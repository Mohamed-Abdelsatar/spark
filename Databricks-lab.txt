display(dbutils.fs.ls("/databricks-datasets/samples/docs/"))

textFile = spark.read.text("/databricks-datasets/samples/docs/README.md")

textFile.count()

------------------Word Count using scala-------------------

%scala
val lines = sc.textFile("/databricks-datasets/samples/docs/README.md")
val counts = lines.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)

%scala
counts.collect


========================DataFrame====================================

%python
# Use the Spark CSV datasource with options specifying:
# - First line of file is a header
# - Automatically infer the schema of the data
data = spark.read.format("csv") \
  .option("header", "true") \
  .option("inferSchema", "true") \
  .load("/databricks-datasets/samples/population-vs-price/data_geo.csv")

data.count()

data.cache() # Cache data for faster reuse
data = data.dropna() # drop rows with missing values
data.count()

data.take(10)

display(data)

data.createOrReplaceTempView("data_geo")

%sql
select `State Code`, `2015 median sales price` from data_geo

%sql
select City, `2014 Population estimate` from data_geo where `State Code` = 'WA';


----------------visualization--------------------------
%sql
select `State Code`, sum(`2015 median sales price`) as `Sum Sales` from data_geo group by `State Code`

%sql
select City, `2014 Population estimate`/1000 as `2014 Population Estimate (1000s)` from data_geo order by `2015 median sales price` desc limit 10;

========================DataSets====================================
%scala
val range100 = spark.range(100)
range100.collect()

%scala
val df = spark.read.json("/databricks-datasets/samples/people/people.json")


%scala
case class Person (name: String, age: Long)
val ds = spark.read.json("/databricks-datasets/samples/people/people.json").as[Person]


%scala
case class DeviceIoTData (
  battery_level: Long,
  c02_level: Long,
  cca2: String,
  cca3: String,
  cn: String,
  device_id: Long,
  device_name: String,
  humidity: Long,
  ip: String,
  latitude: Double,
  longitude: Double,
  scale: String,
  temp: Long,
  timestamp: Long
)
val ds = spark.read.json("/databricks-datasets/iot/iot_devices.json").as[DeviceIoTData]

%scala
display(ds)

%scala
ds.take(10).foreach(println(_))


%scala
val sorted_device = ds.select($"battery_level", $"c02_level", $"device_name").where($"battery_level" > 6).sort($"c02_level")
display(sorted_device)

%scala
ds.createOrReplaceTempView("iot_device_data")

%sql 
select cca3, count (distinct device_id) as device_id from iot_device_data group by cca3 order by device_id desc limit 100

%scala
ds.write.saveAsTable("iot_device_data")

%scala
ds.write.mode("overwrite").parquet("/tmp/testParquet")

display(dbutils.fs.ls("/tmp/testParquet"))

parquet_directory = spark.read.parquet("/tmp/testParquet/")
display(parquet_directory)

%scala
spark.sql("show tables").show()